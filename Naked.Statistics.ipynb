{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'blue'>Naked Statistics: Stripping the Dread from the Data (Charles Wheelan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are my notes and may contain direct excerpts from the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Introduction\n",
    "- 'The problem is that if the data are poor, or if the statistical techniques are used improperly, the conclusions can be wildly misleading and even potentially dangerous'\n",
    "\n",
    "*Example:* <br>\n",
    "\n",
    "According to a study of 36,000 office workers, those workers who repoorted leaving their offices to take regular ten-minute breaks during the workday were 41% more likely to develop cancer over the next five years, than those who don't leave their offices during the workday.<br>\n",
    "- so, taking short breaks at work causes cancer?\n",
    "- maybe we just need to think more clearly about what many workers are doing during that 10 minute break (many who leave for breaks are huddled outside smoking cigarettes -- creating a haze of smoke through which other break-takers have to walk in order to get in or out of the building)\n",
    "- which is the likely cause of cancer -- taking breaks or smoking?\n",
    "- *'Statistics is like a high-caliber weapon: helpful when used correctly and potentially disastrous in the wrong hands.'*\n",
    "- 'It's easy to lie with statistics, but it's hard to tell the truth wiouth them' -- Andrejs Dunkels, Swedish mathematician and writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 1: What's the Point?\n",
    "\n",
    "Use data from the 'known world' to make informed inferences about the 'unknown world'\n",
    "- summarize huge quantities of data\n",
    "- make better decisions or answer important questions\n",
    "- recognize patterns that can refine how we do things\n",
    "- catch cheaters/criminals\n",
    "- evaluate effectivenessd of policies, programs, products, etc.\n",
    "- know that there are scoundrels out there that will use these powerful tools for nefarious purposes\n",
    "\n",
    "Descriptive statistics exist to simplify, which always implies some loss of nuance or detail. Overreliance on any descriptive statistic can lead to misleading conclusions, or cause undesirable behavior\n",
    "- **Gini index**: standard tool in economics for measuring income inequality, collapses complex information into a single number\n",
    "- **Sampling** is the process of gathering data for a small area and then using those data to make an informed inference about the entire population\n",
    "\n",
    "- **Regression analysis** enables us to isolate a relationship between two variables, while holding constant (or 'controlling for') the effects of other variables. {see notes on Chapter 11}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 2: Descriptive Statistics\n",
    "\n",
    "Descriptive statistics summarize the information in a data set in a meaningful way that make reveal patterns  (i.e., mean, median, standard deviation, percentile scores), however, they do not allow us to make conclusions beyond the data\n",
    "\n",
    "- **percentiles** - divide the distribution into hundredths, this can give you a measure as to how, for example, your score on a test ranks relative to all other test takers.  \n",
    "\n",
    "- **standard deviation** - allows us to assign a single number to describe how far the data are dispersed from their mean  (standard deviation is the square root of the variance) \\sigma\n",
    "\n",
    "- **variance** - calculated by determining how far the observations within a distribution lie from the mean {formula for variance is on page 34}\n",
    "\n",
    "- **normal distribution** - symmetrical around their mean in a bell shape in this way:\n",
    "        - 68.2% are within one standard deviation of the mean()\n",
    "        - 95.4% are within two standard deviations of the mean\n",
    "        - 99.7% are within three standard deviations of the mean\n",
    "        \n",
    "<img src=\"normal.dist.png\" width=\"800\" height=\"400\">\n",
    "\n",
    "Measuring change as a percentage gives us some sense of scale. (remember percent of change is amount of change divided by original amount)\n",
    "\n",
    "The advantage of any index is that it consolidates lots of complex information into a single number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 3: Deceptive Description\n",
    "\n",
    "The overall lesson of this chapter is that statistical malfeasance has very little to do with bad math. If anything, <font color = 'red'> impressive calculations can obscure nefarious motives.  The fact that you've calculated the mean correctly will not alter the fact that the median is the more accurate indicator.  Judgment and integrity turn out to be surprisingly important.<font color = 'black'> A detailed knowledge of statistics does not deter wrongdoing any more than a detailed knowledge of the law averts criminal behavior.\n",
    "    \n",
    "- Pay attention to the unit of analysis\n",
    "- The median is not sensitive to outliers\n",
    "- The median v. mean question revolves around whether the outliers in a distribution distort what is being described or are instead an important part of the message.\n",
    "- Any comprehensive statistical analysis would likely present both the mean and the median.\n",
    "- **real figures** are adjusted for inflation.\n",
    "- **nominal figures** are not adjusted for inflation.    \n",
    "A statistical index has all the potential pitfalls of any descriptive statistic - plus the distortions introduced by combining multiple indicators into a single number. By definition, any index is going to be sensitive to how it is constructed; it will be affected both by what measures go into the index and by how each of those measures is weighted. In the end, the important question is whether the simplicity and ease of use introduced by collapsing many indicators into a single number outweighs the inherent inaccuracy of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 4: Correlation\n",
    "\n",
    "Correlation measures the degree to which two phenomena are related to one another. Two variables are positively correlated if a change is one is associated with a change in the other in the same direction. In a negative association, one increases while the other decreases.\n",
    "\n",
    "- **correlation coefficient** is a single number between -1 and 1\n",
    "- **1 is a perfect positive correlation** (every change in one variable is associated with an equivalent change of the other variable in the same direction)\n",
    "- **-1 is a perfect negative correlation** (every change in one variable is associated with an equivalent change of the other variable in the opposite direction)\n",
    "- the closer a correlation is to 1 or -1, the stronger the association\n",
    "- A correlation close to 0 suggests the variables have no meaningful association with each other\n",
    "\n",
    "**Remember that correlation does not imply causation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 5: Basic Probability\n",
    "Probability is the study of events and outcomes involving an element of uncertainty. They tell us what is likely to happen and what is less likely to happen.\n",
    "- **binomial experiment** - aka Bernoulli trial. Has a fixed number of trials, each with two possible outcomes and the probability of success is the same in each trial.\n",
    "- The probability of two independent events' **both** happening is the **product of their respective probabilities**. In other words, the probability of Event A happening *and* Event B happening is the product of Event A multiplied by Event B.  NOTE: this formula is only applicable if the events are *independent*, meaning the outcome of one has no effect on the outcome of the other.\n",
    "- The probability that one event happens or another event happens:  outcome A *or* outcome B (again, assuming that they are independent).  The probability of getting A or B consists of the sum of their individual probabilities.  (the probability of A *plus* the probability of B).\n",
    "- INDEPENDENT (and = x; or = +)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If the events are not mutually exclusive (if they are not independent), the probability of getting A or B consists of the sum of their individual probabilities minus the proability of both events happening.\n",
    " \n",
    " The expected value or payoff from some event, say purchasing a lottery ticket, is the sum of all the different outcomes, each weighted by its probability and payoff.\n",
    " \n",
    " - Good decisions - as measured by the udnerlying probabilities -- can turn out badly.\n",
    " - And bad decisions can still turn out well, at least in the short run. \n",
    " - But probability triumphs in the end. \n",
    " \n",
    " An important theorem known as the **law of large numbers** tells us that as the number of independent trials increases, the average of the outcomes will get closer and closer to its expected value.\n",
    " - the law of large numbers explains why casinos always make money in the long run.\n",
    " \n",
    "A **probability density function** merely plots the assorted outcomes along the x-axis and the expected probability of each outcome on the y-axis; the weighted probabilities -- each outcome multiplied by its expected frequency -- will add up to 1.\n",
    "\n",
    "'Predictive policing' is part of a broader movement called predictive analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 6: Problems with Probability\n",
    "    \n",
    "**Value at Risk model** (aka VaR) - a common barometer of risk in the financial industry\n",
    "- 'VaR's great appeal, and its great selling point...is that it expresses risk as a single number, a dollar figure, no less' -- Joe Nocera, New York Times business writer\n",
    "    \n",
    "- The crisis that began in 2008 destroyed trillions of dollars in wealth in the United States, drove unemployment over 10%, created waves of home foreclosures and business failures, and saddled governments around the world with huge debts. This is a sadly ironic outcome, given that sophisticated tools like VaR were designed to mitigate risk.\n",
    "    \n",
    "**The most common probability-related errors, misunderstandings and ethical dilemmas:**\n",
    "- **assuming events are independent when they are not**\n",
    "- **not understanding when events ARE independent**. (The very definition of statistical independence between two events is that the outcome of one has no effect on the outcome of the other.)\n",
    "- **Clusters Happen** - when we see an anomalous event out of context, we [wrongly] assume that something besides randomness must be responsible.\n",
    "- **the prosecutor's fallacy**:(the chances of finding a coincidental one in a million match are relatively high if you run the sample through a database with samples from a million people)\n",
    "- **reversion to the mean** (or regression to the mean) - probability tells us that any outlier is likely to be followed by outcomes that are more consistent with the long-term average\n",
    "- **statistical discrimination**: When is it okay to act on the basis of what probability tells us is likely to happen, and when is it not okay? The broader point here is that our ability to analyze data has grown far more sophisticated than our thinking about what we ought to do with the results. For all the elegance and precision of proability, there is no substitute for thinking about what calculations we are doing and why we are doing them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 7:  The Importance of Data ('Garbage in, garbage out')\n",
    "\n",
    "We generally ask our data to do one of three things: \n",
    "1. we demand a sample that is representative of some larger group or population\n",
    "    - requires a simple random sample (each observation in the relevant population has an equal chance of being included in the sample.\n",
    "    - a representative sample enables us to use the most powerful tools statistics has to offer\n",
    "    - many of the most egregious statistical assertions are caused by good statistical methods applied to bad samples, not the opposite\n",
    "    - sample size matters, bigger is better\n",
    "    - if every member of the relevant population does not have an equal chance of ending up in the sample, we are going to have a problem with whatever results emerge from the sample.\n",
    "2. we expect data to provide some source of comparison\n",
    "3. we sometimes have no specific idea what we will do with the information - but we suspect it will come in handy at some point.\n",
    "\n",
    "- **longitudinal data set** - the research equivalent of a Ferrari.  Particularly valuable when it comes to exploring causal relationships that may take years or decades to unfold<br>\n",
    "- **cross-sectional data set** - a collection of data gathered at a single point in time<br>\n",
    "- **self-selection bias** - bias that arises when individuals volunteer to be in a treatment group.<br>\n",
    "- **publication bias** - positive findings are more likely to be published than negative findings, which can skew the results that we see.<br>\n",
    "- **recall bias** - memory is not always a great source of data. Our memories  can be systematically fragile when we are trying to explain some particularly good or bad outcome in the present<br>\n",
    "- **survivorship bias** - occurs when some or many of the observations are falling out of the sample, changing the composition of the observations that are left and therefore affecting the results of any analysis. (If you have a room of people with varying heights, forcing the short people to leave will raise the average height in the room, but it doesn't make anyone taller.)<br>\n",
    "- **healthy user bias** - people who take vitamins regularly are likely to be healthy -- *because they are the kind of people who take vitamins regularly!*<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 8: The Central Limit Theorem\n",
    "\n",
    "The core principle underlying the central limit theorem is that a large, properly drawn sample will resemble the population from which it is drawn. This theorem allows us to make the following inferences: \n",
    "\n",
    "1.  If we have detailed information about some population, then we can make powerful inferences about any properly drawn sample from that population.  According to the  central limit theorem, the average test score for the random sample of 100 students will not typically deviate sharply from the average test score for the whole school.\n",
    "2.  If we have detailed information about a properly drawn sample (mean and standard deviation), we can make strikingly accurate inferences about the population from which that sample was drawn.\n",
    "3. If we have data describing a particular sample, and data on a particular population, we can infer whether or not that sample is consistent with a sample that is likely to be drawn from that population. \n",
    "4. If we know the underlying characteristics of two samples, we can infer whether or not both samples were likely drawn from the same population.\n",
    "\n",
    "**According to the central limit theorem, the sample means for any population will be distributed roughly as a normal distribution around the population mean.**\n",
    "\n",
    "Our best guess for what the mean of any sample will be is the mean of the populations from which it's drawn. The whole point of a representative sample is that it looks like the underlying population.  \n",
    "\n",
    "- As a rule of thumb, the sample size must be at least 30 for the central limit theorem to hold true.\n",
    "\n",
    "### Standard Error (the standard deviation of the sample means)\n",
    "\n",
    "- The standard error measures the dispersion of the sample means.  \n",
    "- Some things to remember: \n",
    "    - The standard deviation measures dispersion in the underlying population\n",
    "    - The standard error measures the dispersion of the sample means\n",
    "    - So the standard error is the standard deviation of the sample means.\n",
    "    \n",
    "- a LARGE standard error means that the sample means are spread out widelly around the population mean\n",
    "- a small standard error means that they are clustered relatively tightly. \n",
    "- Sample means will cluster more tightly around the population mean as the size of each sample gets larger\n",
    "- Sample means will cluster less tightly around the population mean when the underlying population is more spread out\n",
    "\n",
    "**THE BIG PICTURE:**\n",
    "\n",
    "- if you draw large, random samples from any population, the means of those samples will be distributed normally around the population mean (regardless of what the distribution of the underlying population looks like)\n",
    "\n",
    "- most sample means will lie reasonably close to the population mean; the standard error is what defines 'reasonably close'\n",
    "\n",
    "- it is relatively unlikely that a sample mean will lie more than two standard errorsfrom the population mean\n",
    "\n",
    "- the less likely it is that an outcome has been observed by chance, the more confident we can be in surmising that some other factor is in play\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = 'maroon'>Chapter 9: Inference\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
